{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8RZOuS9LWQvv"},"outputs":[],"source":["# import libraries\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  !pip install tf-nightly\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import pandas as pd\n","from tensorflow import keras\n","!pip install tensorflow-datasets\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMHwYXHXCar3"},"outputs":[],"source":["# get data files\n","!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n","!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n","\n","train_file_path = \"train-data.tsv\"\n","test_file_path = \"valid-data.tsv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_h508FEClxO"},"outputs":[],"source":["df_train = pd.read_csv(train_file_path, sep=\"\\t\", header=None, names=['y', 'x'])\n","df_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOMKywn4zReN"},"outputs":[],"source":["df_test = pd.read_csv(test_file_path, sep=\"\\t\", header=None, names=['y', 'x'])\n","df_test.head()"]},{"cell_type":"code","source":["print(len(df_train))\n","print(len(df_test))"],"metadata":{"id":"LHG8PEfIu1MD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train = df_train['y'].astype('category').cat.codes\n","y_test  = df_test['y'].astype('category').cat.codes\n","y_train[:5]"],"metadata":{"id":"6kf8Rzzvu2ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bar = df_train['y'].value_counts()\n","\n","plt.bar(bar.index, bar)\n","plt.xlabel('Label')\n","plt.title('Number of ham and spam messages')"],"metadata":{"id":"w0TQ8EDYu85E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords') # download stopwords\n","nltk.download('wordnet')   # download vocab for lemmatizer"],"metadata":{"id":"wsEKdhPYvB6z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords"],"metadata":{"id":"ckkhuFlKvDiv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stopwords_eng = set(stopwords.words('english'))\n","len(stopwords_eng)"],"metadata":{"id":"x1uNcd8FvIBE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lemmatizer = WordNetLemmatizer()\n","\n","def clean_txt(txt):\n","    txt = re.sub(r'([^\\s\\w])+', ' ', txt)\n","    txt = \" \".join([lemmatizer.lemmatize(word) for word in txt.split()\n","                    if not word in stopwords_eng])\n","    txt = txt.lower()\n","    return txt\n"],"metadata":{"id":"hoTEce4UvLZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = df_train['x'].apply(lambda x: clean_txt(x))\n","X_train[:5]"],"metadata":{"id":"15APCK5DvOoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence"],"metadata":{"id":"8MWI3UpuvT2R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Keep top 1000 frequently occurring words\n","max_words = 1000\n","\n","# Cut off the words after seeing 500 words in each document\n","max_len = 500"],"metadata":{"id":"wst9yJYxvaCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = Tokenizer(num_words=max_words)\n","t.fit_on_texts(X_train)"],"metadata":{"id":"2mOlDQoxvhJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences = t.texts_to_sequences(X_train)\n","sequences[:5]"],"metadata":{"id":"3mC1pnWVvkey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)\n","sequences_matrix[:5]"],"metadata":{"id":"3aYKhTAfvojM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["i = tf.keras.layers.Input(shape=[max_len])\n","x = tf.keras.layers.Embedding(max_words, 50, input_length=max_len)(i)\n","x = tf.keras.layers.LSTM(64)(x)\n","\n","x = tf.keras.layers.Dense(256, activation='relu')(x)\n","x = tf.keras.layers.Dropout(0.5)(x)\n","x = tf.keras.layers.Dense(1, activation='relu')(x)\n","\n","model = tf.keras.models.Model(inputs=i, outputs=x)\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer='RMSprop',\n","    metrics=['accuracy']\n",")\n","model.summary()"],"metadata":{"id":"ziRoOk1zvsX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["r = model.fit(sequences_matrix, y_train,\n","              batch_size=128, epochs=10,\n","              validation_split=0.2,\n","              callbacks=[tf.keras.callbacks.EarlyStopping(\n","                  monitor='val_loss', min_delta=0.0001)])\n"],"metadata":{"id":"LVigZW0mvyfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(r.history['loss'], label='loss')\n","plt.plot(r.history['val_loss'], label='val_loss')\n","plt.legend()\n"],"metadata":{"id":"P5pH52JgwuGx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(r.history['accuracy'], label='acc')\n","plt.plot(r.history['val_accuracy'], label='val_acc')\n","plt.legend()"],"metadata":{"id":"Jo-OVw01w_08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing(X):\n","  x = X.apply(lambda x: clean_txt(x))\n","  x = t.texts_to_sequences(x)\n","  return sequence.pad_sequences(x, maxlen=max_len)"],"metadata":{"id":"YH0_aRbNxEWt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = model.evaluate(preprocessing(df_test['x']), y_test)"],"metadata":{"id":"l9UOkDGkxM46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Loss: {:.3f}, Accuracy: {:.3f}'.format(s[0], s[1]))"],"metadata":{"id":"YtYU4Re2xg-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n","def predict_message(pred_text):\n","  p = model.predict(preprocessing(pd.Series([pred_text])))[0]\n","\n","  return (p[0], (\"ham\" if p<0.5 else \"spam\"))\n","\n","pred_text = \"how are you doing today?\"\n","\n","prediction = predict_message(pred_text)\n","print(prediction)\n"],"metadata":{"id":"T1BkGOZGxix5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dxotov85SjsC"},"outputs":[],"source":["# Run this cell to test your function and model. Do not modify contents.\n","def test_predictions():\n","  test_messages = [\"how are you doing today\",\n","                   \"sale today! to stop texts call 98912460324\",\n","                   \"i dont want to go. can we try it a different day? available sat\",\n","                   \"our new mobile video service is live. just install on your phone to start watching.\",\n","                   \"you have won Â£1000 cash! call to claim your prize.\",\n","                   \"i'll bring it tomorrow. don't forget the milk.\",\n","                   \"wow, is your arm alright. that happened to me one time too\"\n","                  ]\n","\n","  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n","  passed = True\n","\n","  for msg, ans in zip(test_messages, test_answers):\n","    prediction = predict_message(msg)\n","    if prediction[1] != ans:\n","      passed = False\n","\n","  if passed:\n","    print(\"You passed the challenge. Great job!\")\n","  else:\n","    print(\"You haven't passed yet. Keep trying.\")\n","\n","test_predictions()\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/freeCodeCamp/boilerplate-neural-network-sms-text-classifier/blob/master/fcc_sms_text_classification.ipynb","timestamp":1749976370710}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{}},"nbformat":4,"nbformat_minor":0}